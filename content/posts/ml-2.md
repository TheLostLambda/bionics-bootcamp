---
title: "Machine Learning 2: Neural Networks"
date: 2021-03-10T18:04:28+01:00
authors: ["tll"]
categories:
  - Software
tags:
  - Week 4
  - Semester 2
  - Level 2
  - Python
  - Programming
  - Machine Learning
---

Summary
=======

This lesson on neural networks covered:

1. How a simple set of weights and connections can "learn"
2. The basic components of a neural network: weights, biases, layers, activation / loss functions, and gradient descent
3. How CNN networks can process large amounts of spatial / temporal data without a ballooning number of parameters
4. How LSTMs allow neural networks to "remember" and pick up on distant trends in temporal data
5. How these neural networks can actually be constructed in Keras / TensorFlow
6. How activation functions like ReLU and Softmax can be applied to convert the raw output of neurons into positive numbers or probability distributions
7. How cross-entropy loss can capture the degree of "difference" between two distributions (categorical probabilities, in our case)
8. How different optimizers (like Adam) can improve on na√Øve gradient descent performed using a fixed learning rate

Getting Help
============

Even if you've missed the session, you'll be able to ask questions about whatever, whenever on [our Discord](https://discord.gg/N4k7ECt). Feel free to drop your question in the #bootcamp channel or just message an instructor directly.

Useful Links
============

-   [Introduction to Neural Networks](https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9)
-   [Convolutional Neural Networks](https://cs231n.github.io/convolutional-networks/)
-   [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
-   [Creating Models in Keras / Tensorflow](https://www.pyimagesearch.com/2019/10/28/3-ways-to-create-a-keras-model-with-tensorflow-2-0-sequential-functional-and-model-subclassing/)
-   [A Gentle Introduction to ReLU's](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/)
-   [Simple Explanation of Softmax](https://victorzhou.com/blog/softmax/)
-   [Cross Entropy for ML](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)
-   [What is Information Entropy?](https://machinelearningmastery.com/what-is-information-entropy/)
-   [Loss Functions](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)
-   [Gradient Descent Algorithms](https://ruder.io/optimizing-gradient-descent/)

Recording
=========

A bit more abstract than most of our sessions have been, but hopefully you find truly understanding neural networks as rewarding as I do!

{{< youtube vQxJPg8z09I >}}

Feedback
========

Don't forget to fill in [this week's survey](https://forms.gle/FCsZZb8DyyXhACFJA)!
